{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69f75da5-6aaf-4622-b7fd-e6bdbb321d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "樣本數（0=非推銷, 1=推銷）: Counter({0: 31048, 1: 1135})\n",
      "Promotion（前 20 更聚焦）=> ['comminity appliances especially', 'appliances especially washer', 'accomidating noisy kitchens', 'kitchens pretty comminity', 'accomidating noisy', 'okay accomidating', 'okay accomidating noisy', 'pretty comminity appliances', 'noisy kitchens', 'noisy kitchens pretty', 'comminity appliances', 'especially washer', 'especially washer dryers', 'appliances especially', 'pretty comminity', 'years issues addressed', 'live quite located', 'forget human things', 'promptly central', 'think restrictions greater']\n",
      "Off-topic（前 20 更聚焦）=> ['did did did', 'orchard crossing', 'experienced columbia', 'activities communicate messages', 'swimming condo activities', 'communicate messages', 'swimming condo', 'condo activities communicate', 'locations facilities swimming', 'locations facilities', 'communicate messages hear', 'pache locations facilities', 'condo activities', 'activities communicate', 'messages hear noise', 'pache locations', 'hear noise floor', 'facilities swimming condo', 'treatment renting columbia', 'treatment renting']\n",
      "✅ 產出 rules_generated.yml\n"
     ]
    }
   ],
   "source": [
    "# --- Promotion vs Rest：品牌詞過濾 + 稀疏矩陣索引修正 + 輸出規則 --- #\n",
    "import re, numpy as np, pandas as pd, yaml\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "PATH = \"../data/raw/_SELECT_A_object_id_A_complex_id_A_vote_reason_id_B_reason_A_dat_202110291714.csv\"\n",
    "TEXT_COL = \"review_text\"   # 若不同請改\n",
    "\n",
    "df = pd.read_csv(PATH)\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# ---------- 標籤：8 vs 非8 ----------\n",
    "df = df[df[\"vote_reason_id\"].notna()].copy()\n",
    "df[\"is_promo\"] = (df[\"vote_reason_id\"] == 8).astype(int)\n",
    "print(\"樣本數（0=非推銷, 1=推銷）:\", Counter(df[\"is_promo\"]))\n",
    "\n",
    "# ---------- 停用詞/黑名單 ----------\n",
    "domain_stop = {\n",
    "    \"apartment\",\"apartments\",\"complex\",\"property\",\"community\",\"management\",\"staff\",\n",
    "    \"people\",\"place\",\"area\",\"experience\",\"review\",\"reviews\",\"resident\",\"residents\",\n",
    "    \"living\",\"leasing\",\"office\",\"unit\",\"building\",\"maintenance\",\"manager\",\"team\"\n",
    "}\n",
    "filler_stop = {\n",
    "    \"comment\",\"comments\",\"said\",\"told\",\"like\",\"love\",\"great\",\"nice\",\"friendly\",\"amazing\",\n",
    "    \"good\",\"bad\",\"awesome\",\"awful\",\"helpful\",\"quiet\",\"loud\",\"money\",\"quot\",\"amp\"\n",
    "}\n",
    "# 地名/設施/品牌樣式（避免被誤認為推銷話術）\n",
    "facility_stop = {\n",
    "    # 常見設施/地標\n",
    "    \"dog\",\"park\",\"dog park\",\"grocery\",\"store\",\"grocery store\",\"pool\",\"gym\",\"parking\",\"rules\",\n",
    "    \"shopping\",\"center\",\"shopping center\",\"school\",\"bus\",\"stop\",\"train\",\"station\",\"downtown\",\"mall\",\n",
    "    # 社區名稱常見後綴\n",
    "    \"square\",\"landing\",\"club\",\"farm\",\"glen\",\"heights\",\"village\",\"meadows\",\"court\",\"estates\",\"homes\",\n",
    "    \"apartments\",\"apartments\",\"at\",\"on\"\n",
    "}\n",
    "# 已觀察到的品牌/專有名詞（依你的輸出補幾個）\n",
    "proper_noun_like = {\n",
    "    \"lawrence\",\"weaver\",\"timber\",\"grady\",\"polo\"\n",
    "}\n",
    "num_noise = {str(i) for i in range(0,1000)}\n",
    "\n",
    "custom_stops = list(set(ENGLISH_STOP_WORDS) | domain_stop | filler_stop | facility_stop | proper_noun_like | num_noise)\n",
    "\n",
    "def clean(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    # 移除會被 regex 直接偵測的元素，避免干擾片語抽取\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\",\" \", s)\n",
    "    s = re.sub(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", \" \", s)\n",
    "    s = re.sub(r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z\\s']\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "texts = df[TEXT_COL].map(clean)\n",
    "\n",
    "# ---------- 只抓話術：bigram/trigram ----------\n",
    "vec = CountVectorizer(\n",
    "    stop_words=custom_stops,\n",
    "    lowercase=True,\n",
    "    ngram_range=(2,3),      # 抓 \"buy now\", \"visit website\"\n",
    "    min_df=5,               # 視資料量可調：特徵太少→降到3；太雜→拉到8~10\n",
    "    max_df=0.6,\n",
    "    token_pattern=r\"(?u)\\b[a-z][a-z']+\\b\"\n",
    ")\n",
    "X = vec.fit_transform(texts)\n",
    "terms = np.array(vec.get_feature_names_out())\n",
    "\n",
    "# ---------- Log-odds with prior（8 vs 非8）----------\n",
    "y_p = df[\"is_promo\"].values.astype(int)\n",
    "mask_p = (y_p == 1)\n",
    "mask_r = (y_p == 0)\n",
    "\n",
    "promo_ct = (X[mask_p].sum(axis=0).A1 + 0.5)  # +0.5 平滑\n",
    "rest_ct  = (X[mask_r].sum(axis=0).A1 + 0.5)\n",
    "promo_total = promo_ct.sum()\n",
    "rest_total  = rest_ct.sum()\n",
    "\n",
    "logit_promo = np.log(promo_ct / (promo_total - promo_ct))\n",
    "logit_rest  = np.log(rest_ct  / (rest_total  - rest_ct))\n",
    "delta = logit_promo - logit_rest   # 越大越像促銷\n",
    "\n",
    "order = np.argsort(delta)[::-1]\n",
    "\n",
    "# 統一的過濾器：去數字/過短/含停用/含設施或品牌樣式\n",
    "bad_tokens = set().union(*(map(set, [w.split() for w in (facility_stop | proper_noun_like)])))\n",
    "def keep(term):\n",
    "    if any(ch.isdigit() for ch in term): return False\n",
    "    toks = term.split()\n",
    "    if len(toks) < 2: return False\n",
    "    if any(t in bad_tokens for t in toks): return False\n",
    "    if any(t in ENGLISH_STOP_WORDS for t in toks): return False\n",
    "    return True\n",
    "\n",
    "promo_terms = [t for t in terms[order] if keep(t)][:60]\n",
    "print(\"Promotion（前 20 更聚焦）=>\", promo_terms[:20])\n",
    "\n",
    "# ---------- Off-topic：2 vs 非2 ----------\n",
    "df[\"is_off\"] = (df[\"vote_reason_id\"] == 2).astype(int)\n",
    "y_o = df[\"is_off\"].values.astype(int)\n",
    "mask_o = (y_o == 1)\n",
    "mask_not_o = (y_o == 0)\n",
    "\n",
    "off_ct     = (X[mask_o].sum(axis=0).A1 + 0.5)\n",
    "notoff_ct  = (X[mask_not_o].sum(axis=0).A1 + 0.5)\n",
    "off_total  = off_ct.sum()\n",
    "notoff_total = notoff_ct.sum()\n",
    "\n",
    "logit_off    = np.log(off_ct    / (off_total    - off_ct))\n",
    "logit_notoff = np.log(notoff_ct / (notoff_total - notoff_ct))\n",
    "delta_off = logit_off - logit_notoff\n",
    "order_off = np.argsort(delta_off)[::-1]\n",
    "off_terms = [t for t in terms[order_off] if keep(t)][:60]\n",
    "print(\"Off-topic（前 20 更聚焦）=>\", off_terms[:20])\n",
    "\n",
    "# ---------- Promotion 強化 regex（硬規則） ----------\n",
    "promotion_patterns = [\n",
    "    r\"http[s]?://\", r\"\\.com\\b\", r\"\\.net\\b\", r\"\\.ru\\b\",\n",
    "    r\"\\bbit\\.ly\\b\", r\"\\btinyurl\\b\",\n",
    "    r\"\\bpromo\\b\", r\"\\bdiscount\\b\", r\"\\bbuy now\\b\", r\"\\bfree trial\\b\",\n",
    "    r\"\\bvisit (my|our) (site|website)\\b\", r\"\\bcontact (me|us)\\b\",\n",
    "    r\"\\bcall (now|us)\\b\", r\"\\boffer ends\\b\",\n",
    "    r\"\\bapply (now|today)\\b\", r\"\\blease (now|today)\\b\",\n",
    "    r\"\\bmove[-\\s]?in special\\b\", r\"\\bspecial offer\\b\",\n",
    "    r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",\n",
    "    r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\"\n",
    "]\n",
    "\n",
    "rules_yaml = {\n",
    "    \"rules\": [\n",
    "        {\n",
    "            \"id\": \"RULE_PROMOTION\",\n",
    "            \"reason_id\": 8,\n",
    "            \"description\": \"Promotional or business content\",\n",
    "            \"keywords\": promo_terms[:30],\n",
    "            \"pattern\": \"|\".join(promotion_patterns),\n",
    "            \"weight\": 0.7,\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"RULE_OFF_TOPIC\",\n",
    "            \"reason_id\": 2,\n",
    "            \"description\": \"Irrelevant or unhelpful content\",\n",
    "            \"keywords\": off_terms[:30],\n",
    "            \"weight\": 0.4,\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "out_path = \"rules_generated.yml\"   # 輸出在 Notebook 同層\n",
    "with open(out_path, \"w\") as f:\n",
    "    yaml.safe_dump(rules_yaml, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "print(f\"✅ 產出 {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36c7f2f5-ec72-4240-ab55-393bb6be8270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promotion（清理後，前 20）=> ['highly recommend', 'special offer', 'move in special', 'move-in special', 'apply now', 'lease today', 'tour today', 'schedule a tour', 'limited time', 'act fast', 'free application', 'application fee waived', 'waived application fee', 'call now', 'contact us', 'buy now', 'free trial', 'refer a friend', 'hassle free', 'added bonus']\n",
      "Off-topic（清理後，前 20）=> ['dogs barking', 'noise level', 'bed bugs', 'moving soon', 'stay away', 'doesnt work', 'does not work', 'let me know', 'nothing to do', 'off topic', 'not related', 'waste of time', 'did did did', 'orchard crossing', 'experienced columbia', 'activities communicate messages', 'swimming condo activities', 'communicate messages', 'swimming condo', 'condo activities communicate']\n",
      "✅ 規則已輸出到 rules_generated.yml\n"
     ]
    }
   ],
   "source": [
    "# --- Post-process terms: 清理 → 加白名單 → 輸出乾淨規則 --- #\n",
    "import re, yaml\n",
    "\n",
    "# 1) 黑名單：地名/品牌/設施/家電/噪音拼字（可依結果持續擴充）\n",
    "brand_geo_suffix = {\n",
    "    \"square\",\"landing\",\"club\",\"farm\",\"glen\",\"heights\",\"village\",\"meadows\",\"court\",\"estates\",\"homes\"\n",
    "}\n",
    "proper_nouns_seen = {\"lawrence\",\"weaver\",\"timber\",\"grady\",\"polo\"}  # 你剛剛輸出有看到的\n",
    "facilities = {\n",
    "    \"dog\",\"park\",\"dog park\",\"grocery\",\"store\",\"grocery store\",\"pool\",\"gym\",\"parking\",\"rules\",\n",
    "    \"shopping\",\"center\",\"shopping center\",\"school\",\"bus\",\"stop\",\"train\",\"station\",\"downtown\",\"mall\"\n",
    "}\n",
    "appliance_kitchen = {\n",
    "    \"appliance\",\"appliances\",\"washer\",\"washers\",\"dryer\",\"dryers\",\"kitchen\",\"kitchens\",\"stove\",\"fridge\",\n",
    "    \"microwave\",\"dishwasher\",\"oven\",\"sink\",\"countertop\",\"appliances especially\",\"especially washer\",\"noisy kitchens\"\n",
    "}\n",
    "noise_tokens = {\"apos\",\"comminity\",\"accomidating\",\"okay accomidating\"}  # 明顯錯字/殘字\n",
    "\n",
    "black_tokens = brand_geo_suffix | proper_nouns_seen | facilities | appliance_kitchen | noise_tokens\n",
    "\n",
    "def is_bad_term(term: str) -> bool:\n",
    "    toks = term.split()\n",
    "    # 短、含數字、含停用黑名詞就砍\n",
    "    if len(toks) < 2:\n",
    "        return True\n",
    "    if any(ch.isdigit() for ch in term):\n",
    "        return True\n",
    "    if any(t in black_tokens for t in toks):\n",
    "        return True\n",
    "    # 如果最後一個 token 是品牌/地名後綴，也砍\n",
    "    if toks[-1] in brand_geo_suffix:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 2) 促銷白名單（真正像商業話術的片語）\n",
    "promo_whitelist = [\n",
    "    \"highly recommend\",\"special offer\",\"move in special\",\"move-in special\",\"apply now\",\"lease today\",\n",
    "    \"tour today\",\"schedule a tour\",\"limited time\",\"act fast\",\"free application\",\"application fee waived\",\n",
    "    \"waived application fee\",\"call now\",\"contact us\",\"buy now\",\"free trial\",\"refer a friend\",\"hassle free\",\n",
    "    \"added bonus\",\"best community\",\"stop by\",\"book a tour\"\n",
    "]\n",
    "\n",
    "# 3) Off-topic 白名單（偏題/不可驗證的常見片語；當輔助訊號）\n",
    "off_whitelist = [\n",
    "    \"dogs barking\",\"noise level\",\"bed bugs\",\"moving soon\",\"stay away\",\"doesnt work\",\"does not work\",\n",
    "    \"let me know\",\"nothing to do\",\"off topic\",\"not related\",\"waste of time\"\n",
    "]\n",
    "\n",
    "# 4) 把你模型算到的清單清乾淨，再加白名單\n",
    "def clean_list(cands, whitelist, topk=30):\n",
    "    base = [t for t in cands if not is_bad_term(t)]\n",
    "    # 置頂白名單，再接上清理後的自動詞\n",
    "    merged = list(dict.fromkeys(whitelist + base))  # 去重保序\n",
    "    return merged[:topk]\n",
    "\n",
    "promo_clean = clean_list(promo_terms, promo_whitelist, topk=30)\n",
    "off_clean   = clean_list(off_terms,   off_whitelist,   topk=30)\n",
    "\n",
    "print(\"Promotion（清理後，前 20）=>\", promo_clean[:20])\n",
    "print(\"Off-topic（清理後，前 20）=>\", off_clean[:20])\n",
    "\n",
    "# 5) Promotion 的硬規則 regex（仍保留，拉高 precision）\n",
    "promotion_patterns = [\n",
    "    r\"http[s]?://\", r\"\\.com\\b\", r\"\\.net\\b\", r\"\\.ru\\b\",\n",
    "    r\"\\bbit\\.ly\\b\", r\"\\btinyurl\\b\",\n",
    "    r\"\\bpromo\\b\", r\"\\bdiscount\\b\", r\"\\bbuy now\\b\", r\"\\bfree trial\\b\",\n",
    "    r\"\\bvisit (my|our) (site|website)\\b\", r\"\\bcontact (me|us)\\b\",\n",
    "    r\"\\bcall (now|us)\\b\", r\"\\boffer ends\\b\",\n",
    "    r\"\\bapply (now|today)\\b\", r\"\\blease (now|today)\\b\",\n",
    "    r\"\\bmove[-\\s]?in special\\b\", r\"\\bspecial offer\\b\",\n",
    "    r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",\n",
    "    r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\"\n",
    "]\n",
    "\n",
    "rules_yaml = {\n",
    "    \"rules\": [\n",
    "        {\n",
    "            \"id\": \"RULE_PROMOTION\",\n",
    "            \"reason_id\": 8,\n",
    "            \"description\": \"Promotional or business content\",\n",
    "            \"keywords\": promo_clean,\n",
    "            \"pattern\": \"|\".join(promotion_patterns),\n",
    "            \"weight\": 0.7,\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"RULE_OFF_TOPIC\",\n",
    "            \"reason_id\": 2,\n",
    "            \"description\": \"Irrelevant or unhelpful content\",\n",
    "            \"keywords\": off_clean,\n",
    "            \"weight\": 0.35,  # ← Off-topic 比較抽象，先給低一點\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "out_path = \"rules_generated.yml\"  # 留在 Notebook 同層\n",
    "with open(out_path, \"w\") as f:\n",
    "    yaml.safe_dump(rules_yaml, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "print(f\"✅ 規則已輸出到 {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d550234c-bf7f-4cd7-b5c1-902721e9c222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gz/bhmz7vj94s7207h9k2_rgndc0000gn/T/ipykernel_77681/3435122270.py:83: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask = mask | t.str.contains(pat, regex=True, na=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== toxic cleaned top 20 ===\n",
      " ['kill you', 'hurt you', 'harm you', 'go to hell', 'call you names', 'hate you', 'racist', 'sexist', 'pervert', 'creep', 'harass', 'harassment', 'threat', 'violent', 'violence', 'bitch', 'asshole', 'fuck', 'moron', 'idiot'] \n",
      "\n",
      "=== privacy cleaned top 20 ===\n",
      " ['private info', 'personal info', 'share address', 'shared phone number', 'social security', 'license plate', 'full name', 'email address', 'home address', 'leaked info', 'personal information', 'handle personal', 'private information', 'soss', 'work event', 'carelessness', 'house drunkard', 'try importantly', 'files knows', 'sentimentally'] \n",
      "\n",
      "=== covid cleaned top 20 ===\n",
      " ['covid', 'coronavirus', 'pandemic', 'mask mandate', 'quarantine', 'contact tracing', 'vaccine', 'vaccination', 'unvaccinated', 'social distancing', 'lockdown', 'explain conditions', 'pandemic going', 'serve emails', 'said administrative', 'cause failed', 'quote went', 'originally confirmed', 'quote cause', 'went honestly'] \n",
      "\n",
      "✅ 產出 rules_lexicons.yml（seed→擴充後）\n"
     ]
    }
   ],
   "source": [
    "# ==== Seeded bootstrapping for 6/7/9 lexicons ====\n",
    "import re, numpy as np, pandas as pd, yaml\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "PATH = \"../data/raw/_SELECT_A_object_id_A_complex_id_A_vote_reason_id_B_reason_A_dat_202110291714.csv\"\n",
    "TEXT_COL = \"review_text\"\n",
    "\n",
    "df = pd.read_csv(PATH)\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str)\n",
    "\n",
    "def clean_text(s):\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\",\" \", s)\n",
    "    s = re.sub(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", \" \", s)\n",
    "    s = re.sub(r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z\\s']\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "df[\"clean\"] = df[TEXT_COL].map(clean_text)\n",
    "\n",
    "# ---- 種子（可再擴）----\n",
    "SEEDS = {\n",
    "    6: {  # Toxic / Threats / Hate\n",
    "        \"kw\": [\n",
    "            \"kill you\",\"hurt you\",\"harm you\",\"go to hell\",\"call you names\",\"hate you\",\n",
    "            \"racist\",\"sexist\",\"pervert\",\"creep\",\"harass\",\"harassment\",\"threat\",\"violent\",\"violence\",\n",
    "            \"bitch\",\"asshole\",\"fuck\",\"wtf\",\"moron\",\"idiot\"\n",
    "        ],\n",
    "        \"rgx\": [\n",
    "            r\"\\bfuck\\b\", r\"\\bbitch\\b\", r\"\\basshole\\b\",\n",
    "            r\"\\bgo to hell\\b\", r\"\\bkill (you|him|her)\\b\", r\"\\bhate (you|them)\\b\",\n",
    "            r\"\\b(threat|harass|harassment)\\b\"\n",
    "        ]\n",
    "    },\n",
    "    7: {  # Privacy / Personal info\n",
    "        \"kw\": [\n",
    "            \"private info\",\"personal info\",\"share address\",\"shared phone number\",\"social security\",\n",
    "            \"ssn\",\"license plate\",\"full name\",\"email address\",\"home address\",\"leaked info\"\n",
    "        ],\n",
    "        \"rgx\": [\n",
    "            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",        # SSN-like\n",
    "            r\"\\b\\d{3}-\\d{3}-\\d{4}\\b\",        # phone\n",
    "            r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\",\n",
    "            r\"\\bapt\\s?#?\\d+\\b\", r\"\\bunit\\s?#?\\d+\\b\", r\"\\broom\\s?#?\\d+\\b\"\n",
    "        ]\n",
    "    },\n",
    "    9: {  # COVID / Pandemic\n",
    "        \"kw\": [\n",
    "            \"covid\",\"coronavirus\",\"pandemic\",\"mask mandate\",\"quarantine\",\"contact tracing\",\n",
    "            \"vaccine\",\"vaccination\",\"unvaccinated\",\"social distancing\",\"lockdown\"\n",
    "        ],\n",
    "        \"rgx\": [\n",
    "            r\"\\bcovid\\b\", r\"\\bcovid-19\\b\", r\"\\bcoronavirus\\b\",\n",
    "            r\"\\b(mask|masks|mask mandate)\\b\", r\"\\bvaccine\\b\", r\"\\bvaccin(e|ation|ated)\\b\",\n",
    "            r\"\\bquarantine\\b\", r\"\\blockdown\\b\", r\"\\bsocial distancing\\b\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---- 黑名單（更強）----\n",
    "brand_geo = {\n",
    "    \"square\",\"landing\",\"club\",\"farm\",\"glen\",\"heights\",\"village\",\"meadows\",\"court\",\"estates\",\"homes\",\n",
    "    \"lawrence\",\"weaver\",\"timber\",\"grady\",\"polo\",\"orchard\",\"wilshire\",\"promenade\",\"columbia\",\"medina\",\"sykesville\",\"curran\", \"mrs\", \"realestate\", \"cf\", \"wilshire\", \"promenade\"\n",
    "}\n",
    "facilities = {\n",
    "    \"dog\",\"park\",\"dog park\",\"grocery\",\"store\",\"grocery store\",\"pool\",\"gym\",\"parking\",\"rules\",\n",
    "    \"school\",\"bus\",\"stop\",\"train\",\"station\",\"downtown\",\"mall\",\"stairs\",\"hallway\",\"landscaping\"\n",
    "}\n",
    "appliances = {\"appliance\",\"appliances\",\"washer\",\"washers\",\"dryer\",\"dryers\",\"kitchen\",\"kitchens\",\"fridge\",\"microwave\",\"oven\",\"sink\"}\n",
    "typo_noise = {\"apos\",\"comminity\",\"accomidating\",\"ack\",\"lim\",\"gaddy\",\"half\", \"ladies\"}\n",
    "lease_noise = {\"accepted\", \"approved\", \"terms\", \"deposit\", \"unavailable\", \"mention\", \"break\", \"plan\"}\n",
    "custom_stops = list(set(ENGLISH_STOP_WORDS) | brand_geo | facilities | appliances | typo_noise | lease_noise)\n",
    "\n",
    "def make_seed_mask(texts, seeds):\n",
    "    t = texts.str.lower()\n",
    "    mask = pd.Series(False, index=texts.index)\n",
    "    # keyword 直接包含\n",
    "    for k in seeds[\"kw\"]:\n",
    "        mask = mask | t.str.contains(re.escape(k))\n",
    "    # regex\n",
    "    for pat in seeds[\"rgx\"]:\n",
    "        mask = mask | t.str.contains(pat, regex=True, na=False)\n",
    "    return mask.values\n",
    "\n",
    "# 統一的抽詞函式（先 seed → 再 log-odds）\n",
    "def extract_with_seed(label_id, topk=50):\n",
    "    seeds = SEEDS[label_id]\n",
    "    seed_mask = make_seed_mask(df[\"clean\"], seeds)\n",
    "    pos_n = int(seed_mask.sum())\n",
    "    if pos_n < 20:\n",
    "        print(f\"[warn] label {label_id}: seed 命中太少（{pos_n}），請擴充 SEEDS\")\n",
    "    neg_mask = ~seed_mask\n",
    "\n",
    "    # 向量化（unigram + bigram）\n",
    "    vec = CountVectorizer(\n",
    "        stop_words=custom_stops,\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=5,\n",
    "        max_df=0.8,\n",
    "        token_pattern=r\"(?u)\\b[a-z][a-z']+\\b\"\n",
    "    )\n",
    "    X = vec.fit_transform(df[\"clean\"])\n",
    "    terms = np.array(vec.get_feature_names_out())\n",
    "\n",
    "    # log-odds（seed 命中的集合 vs 其他）\n",
    "    ct_pos = X[seed_mask].sum(axis=0).A1 + 0.5\n",
    "    ct_neg = X[neg_mask].sum(axis=0).A1 + 0.5\n",
    "    tot_pos, tot_neg = ct_pos.sum(), ct_neg.sum()\n",
    "    logit_pos = np.log(ct_pos / (tot_pos - ct_pos))\n",
    "    logit_neg = np.log(ct_neg / (tot_neg - ct_neg))\n",
    "    delta = logit_pos - logit_neg\n",
    "    order = np.argsort(delta)[::-1]\n",
    "\n",
    "    # 過濾：去數字、去專有名詞/設施/家電、不要極短/殘片\n",
    "    bad_tokens = brand_geo | facilities | appliances | typo_noise\n",
    "    def keep(term):\n",
    "        if any(ch.isdigit() for ch in term): return False\n",
    "        toks = term.split()\n",
    "        if len(toks) == 1 and len(toks[0]) <= 2: return False\n",
    "        if any(t in bad_tokens for t in toks): return False\n",
    "        return True\n",
    "\n",
    "    picked = [terms[i] for i in order if keep(terms[i])]\n",
    "    # 把 seeds 的 kw 置頂，當白名單，然後接上擴充詞\n",
    "    whitelist = [k for k in seeds[\"kw\"] if \" \" in k or len(k) >= 4]  # 片語優先\n",
    "    merged = list(dict.fromkeys(whitelist + picked))\n",
    "    return merged[:topk]\n",
    "\n",
    "lexicons = {\n",
    "    \"toxic\": extract_with_seed(6, topk=60),\n",
    "    \"privacy\": extract_with_seed(7, topk=60),\n",
    "    \"covid\": extract_with_seed(9, topk=60)\n",
    "}\n",
    "\n",
    "for nm in lexicons:\n",
    "    print(f\"=== {nm} cleaned top 20 ===\\n\", lexicons[nm][:20], \"\\n\")\n",
    "\n",
    "# 建議 regex（強證據）\n",
    "suggested_patterns = {\n",
    "    \"toxic\": r\"\\b(fuck|bitch|asshole|go to hell|kill (you|him|her)|hate (you|them)|threat|harass|harassment)\\b\",\n",
    "    \"privacy\": r\"(\\b\\d{3}-\\d{2}-\\d{4}\\b|\\b\\d{3}-\\d{3}-\\d{4}\\b|[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}|\\b(apt|unit|room)\\s?#?\\d+\\b)\",\n",
    "    \"covid\": r\"\\b(covid|covid-19|coronavirus|mask|mask mandate|vaccine|vaccin(e|ation|ated)|quarantine|lockdown|social distancing)\\b\"\n",
    "}\n",
    "\n",
    "# 匯出\n",
    "out = {\"lexicons\": {}}\n",
    "for nm in [\"toxic\",\"privacy\",\"covid\"]:\n",
    "    out[\"lexicons\"][nm] = {\n",
    "        \"keywords\": lexicons[nm][:30],\n",
    "        \"suggested_pattern\": suggested_patterns[nm]\n",
    "    }\n",
    "\n",
    "with open(\"rules_lexicons.yml\", \"w\") as f:\n",
    "    yaml.safe_dump(out, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "print(\"✅ 產出 rules_lexicons.yml（seed→擴充後）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac774f-e0ba-40c9-bdd8-61bca2f24507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (.venv311)",
   "language": "python",
   "name": "ai311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
